# LLM Proxy Edge Function

## Description
Cette Edge Function sert de proxy s√©curis√© pour les appels aux APIs LLM. Elle g√®re :
- L'authentification utilisateur
- La v√©rification des quotas
- Le stockage s√©curis√© des cl√©s API
- L'enregistrement de l'utilisation

## D√©ploiement

### Pr√©requis
1. Installer Supabase CLI :
   ```bash
   npm install -g supabase
   ```

2. Se connecter √† votre projet :
   ```bash
   supabase login
   supabase link --project-ref <your-project-ref>
   ```

### D√©ployer la fonction
```bash
supabase functions deploy llm-proxy
```

### Variables d'environnement
Les variables suivantes sont automatiquement disponibles :
- `SUPABASE_URL`
- `SUPABASE_SERVICE_ROLE_KEY`

Si vous avez besoin d'ajouter des variables customis√©es :
```bash
supabase secrets set MY_SECRET=value
```

## Utilisation

Depuis le client (React), appelez la fonction via :
```typescript
const { data, error } = await supabase.functions.invoke('llm-proxy', {
  body: {
    action: 'test',
    providerId: 'uuid-du-modele',
    prompt: 'Bonjour, qui es-tu ?',
  },
});
```

## Providers support√©s
- ‚úÖ OpenAI (GPT-3.5, GPT-4, etc.)
- üöß Anthropic (Claude) - √Ä impl√©menter
- üöß Mistral AI - √Ä impl√©menter
- üöß Ollama (Local) - √Ä impl√©menter

## S√©curit√©
- Les cl√©s API ne sont JAMAIS expos√©es au client
- L'authentification est v√©rifi√©e via JWT
- Les quotas sont v√©rifi√©s avant chaque appel
- Toutes les requ√™tes sont logu√©es
