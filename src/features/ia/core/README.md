# IA Core Framework Documentation

## ğŸ¯ Philosophie

Le framework IA Core est conÃ§u pour centraliser toute la logique de gestion des IA dans une architecture modulaire, robuste et extensible. Il Ã©limine la duplication de code et fournit un systÃ¨me unifiÃ© pour tous les outils IA actuels et futurs.

### Principes clÃ©s

- **SÃ©paration des responsabilitÃ©s** : Chaque fichier a un rÃ´le unique et bien dÃ©fini
- **RÃ©utilisabilitÃ©** : Un seul hook `useLLMEngine` pour tous les outils IA
- **Typage fort** : TypeScript strict pour Ã©viter les erreurs
- **Gestion d'erreurs unifiÃ©e** : Tous les types d'erreurs sont catÃ©gorisÃ©s et formatÃ©s de maniÃ¨re cohÃ©rente
- **Performance** : Caching et optimisations intÃ©grÃ©es

## ğŸ“ Structure du Core

```
src/features/ia/core/
â”œâ”€â”€ types.ts              # Types partagÃ©s (LLMRequest, LLMResponse, etc.)
â”œâ”€â”€ promptBuilder.ts      # Construction centralisÃ©e des prompts
â”œâ”€â”€ estimateTokens.ts     # Estimation de tokens avec cache
â”œâ”€â”€ jsonParser.ts         # Parsing JSON robuste (strict/permissif)
â”œâ”€â”€ llmErrorHandler.ts    # Gestionnaire d'erreurs centralisÃ©
â”œâ”€â”€ useLLMEngine.ts       # Hook principal pour appels LLM
â””â”€â”€ index.ts              # Barrel export
```

## ğŸš€ CrÃ©er un nouvel outil IA

### Ã‰tape 1 : CrÃ©er le fichier prompt

CrÃ©er un fichier dans `src/features/ia/prompt/` :

```typescript
// src/features/ia/prompt/monOutil.prompt.ts
import { PromptPart } from '../core/types';

export interface MonOutilPromptInput {
  // Vos paramÃ¨tres d'entrÃ©e
  param1: string;
  param2: number;
}

export function buildMonOutilPrompt(input: MonOutilPromptInput): PromptPart {
  const { param1, param2 } = input;

  return {
    context: `Contexte pour l'outil...`,
    instructions: `Instructions pour le LLM...`,
    // optionnel: maxTokens, system, examples
  };
}
```

**RÃ¨gles importantes** :
- âŒ Pas de logique mÃ©tier
- âŒ Pas d'estimation de tokens
- âŒ Pas d'appels LLM
- âœ… Uniquement du contenu textuel

### Ã‰tape 2 : CrÃ©er le hook personnalisÃ©

```typescript
// src/features/ia/tools/MonOutil/useMonOutil.ts
import { useLLMEngine, buildPrompt } from '../../core';
import { buildMonOutilPrompt } from '../../prompt/monOutil.prompt';

export function useMonOutil() {
  const llmEngine = useLLMEngine<ReponseType>();

  const executerMonOutil = async (
    providerId: string,
    parametres: MonOutilPromptInput
  ) => {
    // 1. Construire le prompt
    const promptParts = buildMonOutilPrompt(parametres);
    const finalPrompt = buildPrompt(promptParts);

    // 2. Appeler le LLM
    const result = await llmEngine.runLLM(
      {
        providerId,
        messages: [{ role: 'user', content: finalPrompt }],
        origin: 'IA_MonOutil',
        maxTokens: 1000
      },
      {
        mode: 'permissive', // ou 'strict'
        arrayField: 'results' // si besoin de rÃ©cupÃ©ration partielle
      }
    );

    // 3. Traiter le rÃ©sultat
    if (result.ok) {
      return result.data;
    } else {
      throw new Error(result.message);
    }
  };

  return {
    executerMonOutil,
    loading: llmEngine.loading,
    error: llmEngine.error,
    usage: llmEngine.usage
  };
}
```

### Ã‰tape 3 : CrÃ©er le composant UI

```typescript
// src/features/ia/tools/MonOutil/MonOutilModal.tsx
import { useMonOutil } from './useMonOutil';

export function MonOutilModal() {
  const { executerMonOutil, loading, error } = useMonOutil();

  const handleExecute = async () => {
    try {
      const result = await executerMonOutil(providerId, parametres);
      // Traiter le rÃ©sultat
    } catch (err) {
      // GÃ©rer l'erreur
    }
  };

  return (
    // Votre UI
    {error && <div>{error.message}</div>}
  );
}
```

## ğŸ§® Activer l'estimation de tokens

Pour afficher une estimation en temps rÃ©el (ex: slider de quantitÃ©) :

```typescript
import { estimatePromptTokens } from '../../core';

// Dans votre composant
const [estimatedTokens, setEstimatedTokens] = useState(0);

useEffect(() => {
  const promptParts = buildMonOutilPrompt(parametres);
  const finalPrompt = buildPrompt(promptParts);
  
  const estimation = estimatePromptTokens(
    'gemini-1.5-flash-002',
    finalPrompt,
    nombreElementsAttendus
  );
  
  setEstimatedTokens(estimation.totalEstimated);
}, [parametres]);
```

## ğŸ” Appeler le LLM proprement

### Configuration de base

```typescript
const result = await llmEngine.runLLM(
  {
    providerId: string,         // ID du provider actif
    messages: Message[],        // Format standard OpenAI
    origin: string,             // Nom de l'outil (pour tracking)
    maxTokens?: number,         // Limite de tokens (optionnel)
    model?: string,             // ModÃ¨le spÃ©cifique (optionnel)
    action?: string             // Type d'action (dÃ©faut: 'generate')
  },
  {
    mode: 'strict' | 'permissive',  // Mode de parsing
    arrayField?: string              // Champ array pour rÃ©cupÃ©ration partielle
  }
);
```

### Gestion des rÃ©sultats

```typescript
if (result.ok) {
  // SuccÃ¨s
  const data = result.data;
  const usage = result.usage; // { prompt_tokens, completion_tokens, total_tokens }
} else {
  // Erreur catÃ©gorisÃ©e
  const type = result.type;    // 'openrouter' | 'network' | 'quota' | 'parse' | 'unknown'
  const message = result.message;
  const raw = result.raw;      // RÃ©ponse brute si erreur de parsing
}
```

## ğŸ“‹ Conventions globales

### RÃ¨gles JSON pour le LLM

AppliquÃ©es automatiquement par `buildPrompt` :

```
Return ONLY valid JSON.
No comments.
No trailing text.
No markdown.
Do not break JSON into several chunks.
If you are reaching the token limit, you MUST:
- complete the current JSON object
- close arrays and objects
- STOP IMMEDIATELY
```

### Gestion d'erreurs

Toutes les erreurs sont catÃ©gorisÃ©es :

| Type | Description |
|------|-------------|
| `openrouter` | Erreurs du provider IA (rate limits, API down) |
| `network` | Erreurs rÃ©seau (connexion, timeout) |
| `quota` | Quota de tokens dÃ©passÃ© |
| `parse` | Erreur de parsing JSON |
| `supabase` | Erreur edge function Supabase |
| `unknown` | Erreur non catÃ©gorisÃ©e |

## ğŸ§ª Parsing JSON

### Mode strict

```typescript
const result = parseJSON<MonType>(rawResponse, { mode: 'strict' });
// Ã‰choue immÃ©diatement si JSON invalide
```

### Mode permissif (recommandÃ©)

```typescript
const result = parseJSON<MonType>(rawResponse, { 
  mode: 'permissive',
  arrayField: 'suggestions' // Active la rÃ©cupÃ©ration partielle
});
// Tente de corriger et rÃ©cupÃ©rer des objets partiels
```

### RÃ©cupÃ©ration partielle

Si le LLM atteint la limite de tokens et coupe le JSON :

```json
{
  "suggestions": [
    {"title": "Item 1", "url": "..."},
    {"title": "Item 2", "url": 
```

Le parser rÃ©cupÃ©rera automatiquement les objets complets (ici: Item 1).

## ğŸ’¡ Exemple complet minimal

```typescript
// 1. Prompt file
export function buildSimplePrompt(input: { query: string }): PromptPart {
  return {
    context: `Query: ${input.query}`,
    instructions: 'RÃ©ponds en JSON: { "answer": "..." }'
  };
}

// 2. Hook
export function useSimpleTool() {
  const llm = useLLMEngine<{ answer: string }>();
  
  const execute = async (query: string, providerId: string) => {
    const prompt = buildPrompt(buildSimplePrompt({ query }));
    const result = await llm.runLLM({
      providerId,
      messages: [{ role: 'user', content: prompt }],
      origin: 'IA_SimpleTool'
    });
    
    return result.ok ? result.data : null;
  };
  
  return { execute, loading: llm.loading, error: llm.error };
}

// 3. Composant
export function SimpleTool() {
  const { execute, loading, error } = useSimpleTool();
  
  const handleClick = async () => {
    const data = await execute('test', providerId);
    console.log(data?.answer);
  };
  
  return <button onClick={handleClick}>Execute</button>;
}
```

## ğŸ“ Bonnes pratiques

1. **Toujours utiliser `useLLMEngine`** pour les appels LLM
2. **SÃ©parer prompt et logique** : un fichier `.prompt.ts` par outil
3. **Typer fortement** : dÃ©finir les interfaces de rÃ©ponse LLM
4. **Mode permissif en prod** : permet la rÃ©cupÃ©ration partielle
5. **Estimer les tokens** : afficher Ã  l'utilisateur avant gÃ©nÃ©ration
6. **CatÃ©goriser les erreurs** : utiliser `error.type` pour l'UX
7. **Logger intelligemment** : garder les console.log pour debugging

## ğŸ”§ Maintenance

### Ajouter un nouveau type d'erreur

Modifier `llmErrorHandler.ts` :

```typescript
// Ajouter le type dans types.ts
export type LLMError = {
  type: '...' | 'nouveau_type';
  // ...
};

// Ajouter le handler
export function handleNouveauTypeError(): LLMError {
  return { ok: false, type: 'nouveau_type', message: '...' };
}
```

### Optimiser l'estimation de tokens

Le cache dans `estimateTokens.ts` garde les encodings en mÃ©moire.
Pour libÃ©rer :

```typescript
import { clearEncodingCache } from '@/features/ia/core';

// En cleanup (ex: unmount)
clearEncodingCache();
```

## ğŸ“Š Architecture simplifiÃ©e

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Composant  â”‚
â”‚     UI      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ utilise
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Hook      â”‚â—„â”€â”€â”€â”€ buildPrompt({ ...buildToolPrompt(...) })
â”‚  useTool()  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ appelle
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ useLLMEngineâ”‚â—„â”€â”€â”€â”€ estimateTokens(...)
â”‚             â”‚â—„â”€â”€â”€â”€ parseJSON(...)
â”‚             â”‚â—„â”€â”€â”€â”€ handleLLMError(...)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ invoke
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  llm-proxy  â”‚
â”‚   (Edge)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Questions ? Bugs ?** Contactez l'Ã©quipe DevOps ou consultez le code source dans `src/features/ia/core/`.
